services:
  # ========================================
  # 1. DATABASES
  # ========================================
  
  # PostgreSQL (Warm Storage - Analytics)
  db:
    image: postgres:15-alpine
    restart: always
    environment:
      POSTGRES_USER: rafay
      POSTGRES_PASSWORD: rafay
      POSTGRES_DB: game_analytics
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rafay -d game_analytics"]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # 1. This runs first (00) to create the airflow DB
      - ./sql/00_init_airflow.sh:/docker-entrypoint-initdb.d/00_init_airflow.sh
      # 2. This runs second (init) for Spark tables
      - ./apps/spark-processor/init.sql:/docker-entrypoint-initdb.d/init.sql
      # 3. This runs third (01) for the Star Schema
      - ./sql/01_star_schema.sql:/docker-entrypoint-initdb.d/01_star_schema.sql
    networks:
      - game_analytics_net

  # MongoDB (Hot Storage - Real-time Data)
  mongo:
    image: mongo:4.0
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin
      MONGO_INITDB_DATABASE: game_analytics
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
      - ./mongo-init:/docker-entrypoint-initdb.d
    networks:
      - game_analytics_net
    healthcheck:
      # Changed 'mongosh' to 'mongo' and updated the eval syntax
      test: mongo --username admin --password admin --authenticationDatabase admin --quiet --eval "db.adminCommand('ping')"
      interval: 10s
      timeout: 5s
      retries: 5

  # Hadoop Namenode (Cold Archive)
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=game_analytics_cluster
    env_file:
      - ./hadoop.env
    networks:
      - game_analytics_net

  # Hadoop Datanode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - game_analytics_net
    depends_on:
      - namenode

  # ========================================
  # 2. STREAMING & MESSAGING
  # ========================================
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - game_analytics_net

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    networks:
      - game_analytics_net

  # ========================================
  # 3. ORCHESTRATION (AIRFLOW)
  # ========================================
  
  airflow-webserver:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    user: root
    restart: always
    depends_on:
      - db
      - mongo
      - kafka
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://rafay:rafay@db:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW_CONN_MONGO_DEFAULT=mongodb://admin:admin@mongo:27017/game_analytics?authSource=admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./packages/game_library:/packages/game_library
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db init &&
               airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
               airflow webserver"
    networks:
      - game_analytics_net
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    user: root
    restart: always
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://rafay:rafay@db:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: airflow scheduler
    networks:
      - game_analytics_net

  # ========================================
  # 4. APPLICATION SERVICES
  # ========================================
  
  # FastAPI Backend
  api:
    build: 
      context: .
      dockerfile: apps/api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://rafay:rafay@db:5432/game_analytics
      - MONGODB_URL=mongodb://admin:admin@mongo:27017/game_analytics?authSource=admin
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    depends_on:
      - db
      - mongo
      - kafka
    volumes:
      - ./apps/api:/app
      - ./packages/game_library:/packages/game_library
    networks:
      - game_analytics_net

  # Data Generator
  faker:
    build: 
      context: .
      dockerfile: apps/faker/Dockerfile
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - MONGODB_URL=mongodb://admin:admin@mongo:27017/game_analytics?authSource=admin
    depends_on:
      - kafka
      - mongo
    volumes:
      - ./apps/faker:/app
      - ./packages/game_library:/packages/game_library
    networks:
      - game_analytics_net

  # Streamlit Dashboard
  web:
    build: 
      context: .
      dockerfile: apps/web/Dockerfile
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8000
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    depends_on:
      - api
      - kafka
    volumes:
      - ./apps/web:/app
      - ./packages/game_library:/packages/game_library
    networks:
      - game_analytics_net

  # Spark Processor
  spark-processor:
    build:
      context: .
      dockerfile: apps/spark-processor/Dockerfile
    depends_on:
      kafka:
        condition: service_started
      db:
        condition: service_healthy
      mongo:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - DATABASE_URL=jdbc:postgresql://db:5432/game_analytics
      - DATABASE_USER=rafay
      - DATABASE_PASSWORD=rafay
      - MONGODB_URL=mongodb://admin:admin@mongo:27017/game_analytics?authSource=admin
    command: python -u /app/main.py
    volumes:
      - ./apps/spark-processor:/app
    restart: unless-stopped
    networks:
      - game_analytics_net

volumes:
  postgres_data:
  mongo_data:
  hadoop_namenode:
  hadoop_datanode:

networks:
  game_analytics_net:
    driver: bridge